# Statistical ML Diabetes Evaluation

A comprehensive machine learning research project evaluating diabetes classification performance across real and synthetic datasets using multiple machine learning algorithms with statistical validation.

## ğŸ“‹ Project Overview

This project implements a complete experimental workflow to:
- Generate synthetic health indicator data using **CTGAN** and **VAE**
- Train and evaluate **3 machine learning models** (Random Forest, SVM, XGBoost)
- Compare performance across **3 datasets** (Real, GAN-generated, VAE-generated)
- Test **4 parameter configurations** per model
- Perform rigorous **statistical analysis** using Friedman tests and post-hoc comparisons
- Generate comprehensive visualizations and reports

**Total Experiments**: 36 (3 datasets Ã— 3 models Ã— 4 parameter sets)

## ğŸ—‚ï¸ Project Structure

```
Statistical ML Diabetes Evaluation/
â”œâ”€â”€ .git/                                # Git version control
â”œâ”€â”€ .gitignore                           # Git ignore file
â”œâ”€â”€ .venv/                               # Python virtual environment
â”œâ”€â”€ Datasets/
â”‚   â”œâ”€â”€ CTGAN/
â”‚   â”‚   â”œâ”€â”€ synthetic_1.csv              # Scaled CTGAN synthetic data
â”‚   â”‚   â””â”€â”€ synthetic_1_unscaled.csv     # Unscaled CTGAN synthetic data
â”‚   â”œâ”€â”€ VAE/
â”‚   â”‚   â”œâ”€â”€ synthetic_2.csv              # Scaled VAE synthetic data
â”‚   â”‚   â””â”€â”€ synthetic_2_unscaled.csv     # Unscaled VAE synthetic data
â”‚   â””â”€â”€ Real/
â”‚       â””â”€â”€ diabetes_012_health_indicators_BRFSS2015.csv
â”œâ”€â”€ models/                              # All trained models
â”‚   â”œâ”€â”€ RandomForest/                    # Random Forest models (12 total)
â”‚   â”‚   â”œâ”€â”€ D1G1P1_model.pkl to D1G1P4_model.pkl  # Real dataset models (4)
â”‚   â”‚   â”œâ”€â”€ D2G1P1_model.pkl to D2G1P4_model.pkl  # CTGAN dataset models (4)
â”‚   â”‚   â””â”€â”€ D3G1P1_model.pkl to D3G1P4_model.pkl  # VAE dataset models (4)
â”‚   â”œâ”€â”€ SVM/                             # SVM models (12 total)
â”‚   â”‚   â”œâ”€â”€ D1G2P1_model.pkl to D1G2P4_model.pkl  # Real dataset models (4)
â”‚   â”‚   â”œâ”€â”€ D2G2P1_model.pkl to D2G2P4_model.pkl  # CTGAN dataset models (4)
â”‚   â”‚   â””â”€â”€ D3G2P1_model.pkl to D3G2P4_model.pkl  # VAE dataset models (4)
â”‚   â”œâ”€â”€ XGBoost/                         # XGBoost models (12 total)
â”‚   â”‚   â”œâ”€â”€ D1G3P1_model.pkl to D1G3P4_model.pkl  # Real dataset models (4)
â”‚   â”‚   â”œâ”€â”€ D2G3P1_model.pkl to D2G3P4_model.pkl  # CTGAN dataset models (4)
â”‚   â”‚   â””â”€â”€ D3G3P1_model.pkl to D3G3P4_model.pkl  # VAE dataset models (4)
â”‚   â”œâ”€â”€ CTGAN/
â”‚   â”‚   â”œâ”€â”€ ctgan_final.pkl              # Trained CTGAN generator model
â”‚   â”‚   â””â”€â”€ checkpoints/                 # CTGAN training checkpoints
â”‚   â””â”€â”€ VAE/
â”‚       â”œâ”€â”€ vae_final.pkl                # Trained VAE generator model
â”‚       â””â”€â”€ checkpoints/                 # VAE training checkpoints
â”œâ”€â”€ Python_Files/
â”‚   â”œâ”€â”€ main.py                          # Main execution script
â”‚   â”œâ”€â”€ CTGAN_model.py                   # CTGAN synthetic data generator
â”‚   â”œâ”€â”€ VAE_model.py                     # VAE synthetic data generator
â”‚   â”œâ”€â”€ model_training_pipeline.py       # Automated ML training pipeline
â”‚   â”œâ”€â”€ statistical_analysis.py          # Friedman & post-hoc tests
â”‚   â”œâ”€â”€ visualization.py                 # Plotting and visualization
â”‚   â”œâ”€â”€ unscale_synthetic_data.py        # Data preprocessing utilities
â”‚   â””â”€â”€ test_statistical_analysis.py     # Statistical analysis tests
â”œâ”€â”€ Reports/                             # Analysis reports and documentation
â”‚   â””â”€â”€ Experimental_Results_Analysis.ipynb  # Jupyter notebook with comprehensive analysis
â”œâ”€â”€ statistical_results/                 # Statistical analysis results
â”‚   â”œâ”€â”€ friedman_test_results.csv        # Friedman ANOVA test summary
â”‚   â”œâ”€â”€ aggregated_model_results.csv     # Mean metrics per model across datasets
â”‚   â”œâ”€â”€ effect_size_results.csv          # Kendall's W effect sizes
â”‚   â”œâ”€â”€ nemenyi_posthoc_results.csv      # Pairwise comparison results (p-values)
â”‚   â”œâ”€â”€ nemenyi_posthoc_mean_ranks.csv   # Mean ranks for Nemenyi test
â”‚   â”œâ”€â”€ hommel_correction_results.csv    # Multiple comparison corrections
â”‚   â”œâ”€â”€ model_rankings_by_metric.csv     # Model rankings per metric
â”‚   â”œâ”€â”€ performance_differences.csv      # Pairwise performance differences
â”‚   â””â”€â”€ dataset_performance_summary.csv  # Performance breakdown by dataset
â”œâ”€â”€ visualizations/                      # Generated plots and charts
â”‚   â””â”€â”€ comprehensive_comparison.png     # Model performance visualizations
â”œâ”€â”€ Project workflow/                    # Documentation and workflows
â”‚   â”œâ”€â”€ Diagram.excalidraw               # Project workflow diagram
â”‚   â””â”€â”€ Workflow Diagram.excalidraw      # Detailed workflow visualization
â”œâ”€â”€ experiment_results_complete.csv      # Complete experimental results (36 runs)
â””â”€â”€ README.md                            # This file
```

## ğŸ¯ Research Methodology

### Datasets (3)
- **D1 (Real)**: BRFSS 2015 diabetes health indicators dataset
- **D2 (GAN)**: Synthetic data generated using CTGAN
- **D3 (VAE)**: Synthetic data generated using Variational Autoencoder

### Models (3)
- **G1 (Random Forest)**: Ensemble decision tree classifier
- **G2 (SVM)**: Support Vector Machine with RBF/Linear kernels
- **G3 (XGBoost)**: Gradient boosting classifier

### Parameter Sets (4 per model)
Each model group is tested with 4 different parameter configurations (P1-P4):

#### Random Forest Parameters
- **P1**: `n_estimators=50, max_depth=10, min_samples_split=5`
- **P2**: `n_estimators=100, max_depth=20, min_samples_split=10`
- **P3**: `n_estimators=150, max_depth=30, min_samples_split=15`
- **P4**: `n_estimators=200, max_depth=None, min_samples_split=20`

#### SVM Parameters
- **P1**: `C=0.1, kernel=rbf, gamma=scale, max_iter=500`
- **P2**: `C=1.0, kernel=rbf, gamma=scale, max_iter=1000`
- **P3**: `C=10.0, kernel=rbf, gamma=auto, max_iter=1000`
- **P4**: `C=1.0, kernel=linear, max_iter=1000`

#### XGBoost Parameters
- **P1**: `n_estimators=50, learning_rate=0.05, max_depth=3`
- **P2**: `n_estimators=100, learning_rate=0.1, max_depth=6`
- **P3**: `n_estimators=150, learning_rate=0.15, max_depth=9`
- **P4**: `n_estimators=200, learning_rate=0.2, max_depth=12`

### Statistical Analysis

#### Friedman ANOVA Test
- Non-parametric test comparing model groups across dataset blocks
- Tests null hypothesis: all model groups perform equally
- Uses mean performance per group per dataset (3Ã—3 matrix)

#### Post-hoc Nemenyi Test
- Pairwise comparisons between model groups
- Controls family-wise error rate
- Identifies which specific groups differ significantly

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- pip package manager

### Required Libraries

```bash
pip install pandas numpy scikit-learn xgboost
pip install scipy statsmodels scikit-posthocs
pip install matplotlib seaborn
pip install sdv torch joblib
```

### Quick Setup

```bash
# Clone the repository
git clone https://github.com/Mahmoud-Gamal-Elgendy/Statistical-ML-Diabetes-Evaluation.git
cd Statistical-ML-Diabetes-Evaluation
```

## ğŸ’» Usage

### Complete Workflow Execution

Run the entire experimental pipeline:

```bash
cd Python_Files
python main.py
```

This will:
1. Train 36 model configurations
2. Save trained models to `models/` directory
3. Perform statistical analysis
4. Generate visualizations
5. Export results to `experiment_results_complete.csv`

### Individual Components

#### Generate Synthetic Data (CTGAN)

```bash
python CTGAN_model.py
```

#### Generate Synthetic Data (VAE)

```bash
python VAE_model.py
```

#### Run Statistical Analysis Only

```python
from statistical_analysis import run_complete_statistical_analysis
import pandas as pd

results_df = pd.read_csv('experiment_results_complete.csv')
stats = run_complete_statistical_analysis(results_df)
```

#### Generate Visualizations Only

```python
from visualization import save_visualizations
import pandas as pd

results_df = pd.read_csv('experiment_results_complete.csv')
save_visualizations(results_df, output_dir='visualizations')
```


## ğŸ”¬ Key Features

### 1. Automated Training Pipeline
- Seamless training across all dataset-model-parameter combinations
- Automatic model persistence and metadata tracking
- Progress monitoring and verbose logging

### 2. Comprehensive Metrics
- **Accuracy**: Overall classification correctness
- **Precision**: True positive rate
- **Recall**: Sensitivity/true positive detection
- **F1-Score**: Harmonic mean of precision and recall
- **Training Time**: Model training duration

### 3. Statistical Validation
- Friedman ANOVA test for group comparisons
- Nemenyi post-hoc test for pairwise differences
- Multiple testing correction
- Effect size reporting

### 4. Rich Visualizations
- Performance comparison box plots
- Model group comparison charts
- Dataset performance analysis
- Parameter sensitivity plots


## ğŸ§ª Testing

Run tests:

```bash
python test_statistical_analysis.py
```

## ğŸ“ Research Applications

This framework is suitable for:
- Synthetic data quality evaluation
- Model robustness testing across data distributions
- Hyperparameter sensitivity analysis
- Comparative machine learning studies
- Healthcare ML research

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ‘¥ Author

**Mahmoud Youssef**
- GitHub: [@Mahmoud-Gamal-Elgendy](https://github.com/Mahmoud-Gamal-Elgendy)
- Project: Statistical ML Diabetes Evaluation

## ğŸ™ Acknowledgments

- **Dataset**: BRFSS 2015 Diabetes Health Indicators
- **Libraries**: scikit-learn, XGBoost, SDV, PyTorch

## ğŸ“š References

- CTGAN: Conditional Tabular GAN for synthetic data generation
- VAE: Variational Autoencoder for data synthesis
- Friedman Test: Non-parametric ANOVA for repeated measures
- Nemenyi Test: Post-hoc pairwise comparison test

---

**Last Updated**: December 2025  
**Status**: Active Development
