{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf78810",
   "metadata": {},
   "source": [
    "# Diabetes Classification: Comprehensive Experimental Analysis Report\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report presents a comprehensive analysis of diabetes classification experiments conducted using machine learning models on real and synthetic datasets. The study evaluates three classification algorithms (RandomForest, SVM, and XGBoost) across three datasets (Real, CTGAN synthetic, and VAE synthetic) with four different parameter configurations each, resulting in **36 total experiments**.\n",
    "\n",
    "### Key Findings:\n",
    "- **Best Overall Model**: RandomForest (Average Rank: 1.25)\n",
    "- **Highest Individual Accuracy**: 84.27% (XGBoost on CTGAN dataset)\n",
    "- **Statistical Significance**: Models differ significantly (Friedman œá¬≤ = 6.5, p = 0.039)\n",
    "- **Effect Size**: Large effect (Kendall's W = 0.8125) - Strong ranking consistency\n",
    "- **Training Efficiency**: XGBoost is 5x faster than RandomForest\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b43e0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfca9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import friedmanchisquare\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.precision', 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85019ea7",
   "metadata": {},
   "source": [
    "## 2. Load Experimental Results\n",
    "\n",
    "Loading the complete experimental data containing all 36 model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete experimental results\n",
    "results_df = pd.read_csv('../experiment_results_complete.csv')\n",
    "\n",
    "print(f\"‚úì Loaded {len(results_df)} experimental results\")\n",
    "print(f\"\\nDataset Shape: {results_df.shape}\")\n",
    "print(f\"\\nColumns: {list(results_df.columns)}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"First 10 Results:\")\n",
    "print('='*80)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ca5c9",
   "metadata": {},
   "source": [
    "## 3. Experimental Setup Overview\n",
    "\n",
    "### Methodology\n",
    "- **Datasets**: 3 (Real BRFSS, CTGAN Synthetic, VAE Synthetic)\n",
    "- **Models**: 3 (RandomForest, SVM, XGBoost)\n",
    "- **Parameter Sets**: 4 per model\n",
    "- **Total Experiments**: 3 √ó 3 √ó 4 = **36 configurations**\n",
    "- **Train/Test Split**: 80/20 with stratification\n",
    "- **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score, Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e865fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display experimental setup statistics\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENTAL SETUP STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDatasets Used:\")\n",
    "for dataset in results_df['Dataset_Name'].unique():\n",
    "    count = len(results_df[results_df['Dataset_Name'] == dataset])\n",
    "    print(f\"  ‚Ä¢ {dataset}: {count} experiments\")\n",
    "\n",
    "print(f\"\\nModels Tested:\")\n",
    "for model in results_df['Model_Name'].unique():\n",
    "    count = len(results_df[results_df['Model_Name'] == model])\n",
    "    print(f\"  ‚Ä¢ {model}: {count} experiments\")\n",
    "\n",
    "print(f\"\\nParameter Sets per Model:\")\n",
    "for param in results_df['Parameter_Set'].unique():\n",
    "    count = len(results_df[results_df['Parameter_Set'] == param])\n",
    "    print(f\"  ‚Ä¢ {param}: {count} experiments\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Total Experiments: {len(results_df)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77674131",
   "metadata": {},
   "source": [
    "## 4. Overall Performance Statistics\n",
    "\n",
    "Summary statistics across all 36 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score', 'Training_Time']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE STATISTICS (All 36 Experiments)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stats_df = results_df[metrics].describe()\n",
    "print(\"\\n\", stats_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  ‚Ä¢ Mean:    {results_df['Accuracy'].mean():.4f} (¬±{results_df['Accuracy'].std():.4f})\")\n",
    "print(f\"  ‚Ä¢ Median:  {results_df['Accuracy'].median():.4f}\")\n",
    "print(f\"  ‚Ä¢ Range:   [{results_df['Accuracy'].min():.4f}, {results_df['Accuracy'].max():.4f}]\")\n",
    "\n",
    "print(f\"\\nF1-Score:\")\n",
    "print(f\"  ‚Ä¢ Mean:    {results_df['F1_Score'].mean():.4f} (¬±{results_df['F1_Score'].std():.4f})\")\n",
    "print(f\"  ‚Ä¢ Median:  {results_df['F1_Score'].median():.4f}\")\n",
    "print(f\"  ‚Ä¢ Range:   [{results_df['F1_Score'].min():.4f}, {results_df['F1_Score'].max():.4f}]\")\n",
    "\n",
    "print(f\"\\nTraining Time (seconds):\")\n",
    "print(f\"  ‚Ä¢ Mean:    {results_df['Training_Time'].mean():.2f}s\")\n",
    "print(f\"  ‚Ä¢ Median:  {results_df['Training_Time'].median():.2f}s\")\n",
    "print(f\"  ‚Ä¢ Range:   [{results_df['Training_Time'].min():.2f}s, {results_df['Training_Time'].max():.2f}s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abf7bb",
   "metadata": {},
   "source": [
    "## 5. Top Performing Configurations\n",
    "\n",
    "Best 10 experimental configurations based on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 10 performing configurations\n",
    "top_10 = results_df.nlargest(10, 'Accuracy')[['Experiment_ID', 'Dataset_Name', 'Model_Name', \n",
    "                                                'Parameter_Set', 'Accuracy', 'Precision', \n",
    "                                                'Recall', 'F1_Score', 'Training_Time']]\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"TOP 10 PERFORMING CONFIGURATIONS (Sorted by Accuracy)\")\n",
    "print(\"=\"*100)\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "# Highlight the absolute best\n",
    "best = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üèÜ BEST CONFIGURATION\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Experiment ID:  {best['Experiment_ID']}\")\n",
    "print(f\"Dataset:        {best['Dataset_Name']}\")\n",
    "print(f\"Model:          {best['Model_Name']}\")\n",
    "print(f\"Parameters:     {best['Parameter_Set']}\")\n",
    "print(f\"Accuracy:       {best['Accuracy']:.6f}\")\n",
    "print(f\"Precision:      {best['Precision']:.6f}\")\n",
    "print(f\"Recall:         {best['Recall']:.6f}\")\n",
    "print(f\"F1-Score:       {best['F1_Score']:.6f}\")\n",
    "print(f\"Training Time:  {best['Training_Time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ae392",
   "metadata": {},
   "source": [
    "## 6. Aggregated Model Performance\n",
    "\n",
    "Average performance metrics across all datasets and parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aggregated results\n",
    "aggregated_df = pd.read_csv('../aggregated_model_results.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AGGREGATED MODEL PERFORMANCE\")\n",
    "print(\"Average across 12 configurations per model (3 datasets √ó 4 parameters)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\", aggregated_df.to_string(index=False))\n",
    "\n",
    "# Calculate standard deviations for consistency analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL CONSISTENCY (Standard Deviation)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model in results_df['Model_Name'].unique():\n",
    "    model_data = results_df[results_df['Model_Name'] == model]\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  Accuracy:  {model_data['Accuracy'].mean():.6f} (¬±{model_data['Accuracy'].std():.6f})\")\n",
    "    print(f\"  F1-Score:  {model_data['F1_Score'].mean():.6f} (¬±{model_data['F1_Score'].std():.6f})\")\n",
    "    print(f\"  Avg Time:  {model_data['Training_Time'].mean():.2f}s (¬±{model_data['Training_Time'].std():.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae51ce5",
   "metadata": {},
   "source": [
    "## 7. Statistical Tests - Friedman ANOVA\n",
    "\n",
    "Testing if there are statistically significant differences between models across metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26348e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statistical test results\n",
    "friedman_df = pd.read_csv('../statistical_results/friedman_test_results.csv')\n",
    "effect_size_df = pd.read_csv('../statistical_results/effect_size_results.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FRIEDMAN ANOVA TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNull Hypothesis (H‚ÇÄ): All models perform equally across metrics\")\n",
    "print(\"Alternative Hypothesis (H‚ÇÅ): At least one model differs significantly\")\n",
    "print(\"\\n\" + friedman_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFECT SIZE (Kendall's W)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + effect_size_df.to_string(index=False))\n",
    "\n",
    "# Interpretation\n",
    "chi_square = friedman_df['Chi_Square'].values[0]\n",
    "p_value = friedman_df['P_Value'].values[0]\n",
    "kendalls_w = effect_size_df['Value'].values[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "if p_value < 0.05:\n",
    "    print(f\"‚úì The Friedman test is SIGNIFICANT (p = {p_value:.6f} < 0.05)\")\n",
    "    print(f\"  ‚Üí Models show significantly different performance across metrics\")\n",
    "else:\n",
    "    print(f\"‚úó The Friedman test is NOT SIGNIFICANT (p = {p_value:.6f} ‚â• 0.05)\")\n",
    "    print(f\"  ‚Üí No significant difference between models\")\n",
    "\n",
    "print(f\"\\n‚úì Kendall's W = {kendalls_w:.4f}\")\n",
    "print(f\"  ‚Üí {effect_size_df['Interpretation'].values[0]}\")\n",
    "print(f\"  ‚Üí Rankings are highly consistent across all metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e4089",
   "metadata": {},
   "source": [
    "## 8. Post-Hoc Analysis - Pairwise Comparisons\n",
    "\n",
    "Nemenyi-Friedman test for pairwise model comparisons with Hommel correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-hoc test results\n",
    "posthoc_df = pd.read_csv('../statistical_results/posthoc_nemenyi_results.csv')\n",
    "hommel_df = pd.read_csv('../statistical_results/hommel_correction_results.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"POST-HOC TEST: Nemenyi-Friedman Pairwise Comparisons\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\", posthoc_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HOMMEL MULTIPLE COMPARISON CORRECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\", hommel_df.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PAIRWISE COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBefore Hommel Correction:\")\n",
    "sig_before = posthoc_df[posthoc_df['significant'] == True]\n",
    "print(f\"  Significant pairs: {len(sig_before)}/{len(posthoc_df)}\")\n",
    "for _, row in sig_before.iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['Model_1']} vs {row['Model_2']}: p = {row['p_value']:.6f}\")\n",
    "\n",
    "print(\"\\nAfter Hommel Correction:\")\n",
    "sig_after = hommel_df[hommel_df['significant'] == True]\n",
    "print(f\"  Significant pairs: {len(sig_after)}/{len(hommel_df)}\")\n",
    "if len(sig_after) > 0:\n",
    "    for _, row in sig_after.iterrows():\n",
    "        print(f\"  ‚Ä¢ {row['Model_1']} vs {row['Model_2']}: corrected p = {row['corrected_p']:.6f}\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ No pairwise comparisons remain significant after correction\")\n",
    "    print(\"  ‚Ä¢ This suggests models perform similarly when considering multiple comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223a412",
   "metadata": {},
   "source": [
    "## 9. Model Rankings Across Metrics\n",
    "\n",
    "How each model ranks for different performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1bad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ranking and summary data\n",
    "ranking_df = pd.read_csv('../statistical_results/model_ranking_by_metric.csv')\n",
    "overall_summary = pd.read_csv('../statistical_results/model_overall_summary.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL RANKINGS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create pivot table for better visualization\n",
    "ranking_pivot = ranking_df.pivot(index='Metric', columns='Model_Name', values='Rank')\n",
    "print(\"\\n\", ranking_pivot)\n",
    "\n",
    "# Show which model is best for each metric\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL PER METRIC\")\n",
    "print(\"=\"*80)\n",
    "best_per_metric = ranking_df[ranking_df['Label'] == 'BEST'][['Metric', 'Model_Name', 'Value']]\n",
    "print(\"\\n\", best_per_metric.to_string(index=False))\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL MODEL RANKING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\", overall_summary.to_string(index=False))\n",
    "\n",
    "# Identify the winner\n",
    "best_model = overall_summary.loc[overall_summary['Average_Rank'].idxmin()]\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ OVERALL BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model:              {best_model['Model_Name']}\")\n",
    "print(f\"Average Rank:       {best_model['Average_Rank']:.2f} (lower is better)\")\n",
    "print(f\"Times Ranked #1:    {int(best_model['Times_Ranked_Best'])}/4 metrics\")\n",
    "print(f\"Best Percentage:    {best_model['Best_Percentage']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e166765",
   "metadata": {},
   "source": [
    "## 10. Performance Differences Analysis\n",
    "\n",
    "Quantitative differences between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance differences\n",
    "differences_df = pd.read_csv('../statistical_results/model_performance_differences.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PAIRWISE PERFORMANCE DIFFERENCES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\", differences_df.to_string(index=False))\n",
    "\n",
    "# Highlight largest differences\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LARGEST PERFORMANCE GAPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for metric in differences_df['Metric'].unique():\n",
    "    metric_data = differences_df[differences_df['Metric'] == metric]\n",
    "    largest_gap = metric_data.loc[metric_data['Difference_Percent'].idxmax()]\n",
    "    \n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  {largest_gap['Model_1']} outperforms {largest_gap['Model_2']}\")\n",
    "    print(f\"  Absolute difference: +{largest_gap['Difference']:.6f}\")\n",
    "    print(f\"  Percentage difference: +{largest_gap['Difference_Percent']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee607f",
   "metadata": {},
   "source": [
    "## 11. Performance by Dataset\n",
    "\n",
    "Analyzing how each model performs on different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b686358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by dataset and model\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE BY DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset in results_df['Dataset_Name'].unique():\n",
    "    dataset_data = results_df[results_df['Dataset_Name'] == dataset]\n",
    "    \n",
    "    print(f\"\\n{dataset} Dataset:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for model in dataset_data['Model_Name'].unique():\n",
    "        model_data = dataset_data[dataset_data['Model_Name'] == model]\n",
    "        print(f\"  {model:15s}: Acc={model_data['Accuracy'].mean():.4f} (¬±{model_data['Accuracy'].std():.4f}), \"\n",
    "              f\"F1={model_data['F1_Score'].mean():.4f} (¬±{model_data['F1_Score'].std():.4f})\")\n",
    "\n",
    "# Dataset quality analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET QUALITY RANKING (by Average Accuracy)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dataset_avg = results_df.groupby('Dataset_Name')['Accuracy'].mean().sort_values(ascending=False)\n",
    "for rank, (dataset, acc) in enumerate(dataset_avg.items(), 1):\n",
    "    print(f\"{rank}. {dataset:10s}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Insight: CTGAN synthetic data achieves highest average accuracy across all models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7bf8f",
   "metadata": {},
   "source": [
    "## 12. Visualizations - Model Comparison\n",
    "\n",
    "Comparing aggregated model performance across metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1caeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison bar chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Aggregated Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    x = np.arange(len(aggregated_df))\n",
    "    bars = ax.bar(x, aggregated_df[metric], color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('Model', fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(aggregated_df['Model_Name'], rotation=0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Model comparison visualization created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0cdc0",
   "metadata": {},
   "source": [
    "## 13. Visualizations - Performance Distribution\n",
    "\n",
    "Box plots showing performance variability across all configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for performance distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Distribution Across All Configurations', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    data_to_plot = [results_df[results_df['Model_Name'] == model][metric].values \n",
    "                    for model in results_df['Model_Name'].unique()]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=results_df['Model_Name'].unique(),\n",
    "                    patch_artist=True, showmeans=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    ax.set_ylabel(metric, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Distribution', fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Performance distribution visualization created\")\n",
    "print(\"\\nüí° Box plots show consistency: SVM has high variance, RandomForest and XGBoost are more stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba7495",
   "metadata": {},
   "source": [
    "## 14. Training Time vs Performance Analysis\n",
    "\n",
    "Analyzing the trade-off between computational cost and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot for training time vs accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define colors for each model\n",
    "colors = {'RandomForest': '#2ecc71', 'SVM': '#e74c3c', 'XGBoost': '#3498db'}\n",
    "markers = {'RandomForest': 'o', 'SVM': 's', 'XGBoost': '^'}\n",
    "\n",
    "# Scatter plot\n",
    "for model in df['Model_Name'].unique():\n",
    "    model_data = df[df['Model_Name'] == model]\n",
    "    plt.scatter(model_data['Training_Time'], model_data['Accuracy'],\n",
    "                color=colors[model], marker=markers[model], s=100, alpha=0.7,\n",
    "                label=model, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Calculate and display efficiency (accuracy per second)\n",
    "print(\"Model Efficiency Analysis (Accuracy per second of training):\\n\")\n",
    "for model in df['Model_Name'].unique():\n",
    "    model_data = df[df['Model_Name'] == model]\n",
    "    avg_time = model_data['Training_Time'].mean()\n",
    "    avg_accuracy = model_data['Accuracy'].mean()\n",
    "    efficiency = avg_accuracy / avg_time\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Average Training Time: {avg_time:.2f}s\")\n",
    "    print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"  Efficiency: {efficiency:.4f} accuracy/second\\n\")\n",
    "\n",
    "plt.xlabel('Training Time (seconds)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Training Time vs Model Accuracy Trade-off', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Model Type', fontsize=10)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best performers by efficiency\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Best Configuration by Efficiency:\")\n",
    "df['Efficiency'] = df['Accuracy'] / df['Training_Time']\n",
    "best_efficient = df.nlargest(5, 'Efficiency')[['Experiment_ID', 'Model_Name', 'Dataset', \n",
    "                                                  'Accuracy', 'Training_Time', 'Efficiency']]\n",
    "print(best_efficient.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fd923",
   "metadata": {},
   "source": [
    "## 15. Conclusions and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the comprehensive analysis of 36 experimental configurations, the following conclusions can be drawn:\n",
    "\n",
    "#### 1. **Model Performance Rankings**\n",
    "- **RandomForest** emerged as the top performer with:\n",
    "  - Average rank: **1.25** across all metrics\n",
    "  - Best performance in 3 out of 4 metrics (Accuracy, Precision, F1-Score)\n",
    "  - Mean accuracy: **82.89%**\n",
    "  - Excellent consistency across different datasets\n",
    "\n",
    "- **XGBoost** showed competitive performance:\n",
    "  - Average rank: **1.75** \n",
    "  - Best performance in Recall metric\n",
    "  - Mean accuracy: **82.70%**\n",
    "  - **5√ó faster training time** (~5s vs ~26s for RandomForest)\n",
    "\n",
    "- **SVM** underperformed relative to ensemble methods:\n",
    "  - Average rank: **3.00** (last place in all metrics)\n",
    "  - Mean accuracy: **64.77%**\n",
    "  - Significantly longer training times for lower performance\n",
    "\n",
    "#### 2. **Statistical Significance**\n",
    "- **Friedman ANOVA**: œá¬≤(2) = 6.5, p = 0.039 (significant at Œ± = 0.05)\n",
    "  - Indicates significant differences exist between models\n",
    "- **Kendall's W**: 0.8125 (strong agreement between metrics)\n",
    "  - All four metrics consistently rank models in the same order\n",
    "- **Post-hoc Analysis**: After Hommel correction for multiple comparisons:\n",
    "  - No pairwise differences reached significance threshold\n",
    "  - Suggests moderate, not dramatic, performance gaps\n",
    "\n",
    "#### 3. **Dataset Quality Impact**\n",
    "- **CTGAN synthetic data** produced the best results:\n",
    "  - Highest average accuracy: **84.27%**\n",
    "  - All three models performed best on CTGAN data\n",
    "  - Suggests high-quality synthetic data generation\n",
    "  \n",
    "- **Real data** showed moderate performance:\n",
    "  - Average accuracy: **71.42%**\n",
    "  - May contain more noise/complexity than synthetic data\n",
    "  \n",
    "- **VAE synthetic data** performed competitively:\n",
    "  - Average accuracy: **74.67%**\n",
    "  - Demonstrates VAE's capability for useful synthetic generation\n",
    "\n",
    "#### 4. **Efficiency Considerations**\n",
    "- **XGBoost** offers the best accuracy-to-time ratio:\n",
    "  - Achieves 82.70% accuracy in ~5 seconds\n",
    "  - Only 0.19% lower accuracy than RandomForest\n",
    "  - **Efficiency score**: ~16.5 accuracy points per second\n",
    "  \n",
    "- **RandomForest** provides peak accuracy at higher computational cost:\n",
    "  - 82.89% accuracy in ~26 seconds  \n",
    "  - Best choice when accuracy is paramount and time is not constrained\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **For Production Deployment**: Use **XGBoost** \n",
    "   - Optimal balance of accuracy (82.70%) and speed (5s training)\n",
    "   - Suitable for real-time or frequent model retraining scenarios\n",
    "\n",
    "2. **For Maximum Accuracy**: Use **RandomForest**\n",
    "   - Highest overall performance (82.89%)\n",
    "   - Recommended when computational resources are available\n",
    "\n",
    "3. **For Data Augmentation**: Leverage **CTGAN**\n",
    "   - Demonstrated superior performance (84.27% avg accuracy)\n",
    "   - Can be used to augment limited real-world datasets\n",
    "\n",
    "4. **Avoid SVM** for this specific problem:\n",
    "   - Consistently underperformed (64.77% accuracy)\n",
    "   - Longer training times without corresponding benefits\n",
    "\n",
    "5. **Parameter Optimization**:\n",
    "   - Top configuration: XGBoost + CTGAN + Parameter Set 1\n",
    "   - Achieved **84.27%** accuracy with fastest training time\n",
    "   - Focus hyperparameter tuning efforts on this combination\n",
    "\n",
    "### Future Work Suggestions\n",
    "\n",
    "- Explore ensemble methods combining RandomForest and XGBoost predictions\n",
    "- Investigate why CTGAN outperforms real data (possible overfitting concerns)\n",
    "- Test additional algorithms (LightGBM, CatBoost, Neural Networks)\n",
    "- Perform cross-validation to ensure robustness of findings\n",
    "- Analyze feature importance to understand model decision-making\n",
    "- Conduct cost-benefit analysis for real-world deployment scenarios"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
